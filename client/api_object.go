package client

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/hex"
	"encoding/json"
	"encoding/xml"
	"errors"
	"fmt"
	"io"
	"math"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"strings"
	"time"

	ctypes "github.com/cometbft/cometbft/rpc/core/types"

	hashlib "github.com/bnb-chain/greenfield-common/go/hash"
	"github.com/bnb-chain/greenfield-go-sdk/pkg/utils"
	"github.com/bnb-chain/greenfield-go-sdk/types"
	gnfdsdk "github.com/bnb-chain/greenfield/sdk/types"
	gnfdTypes "github.com/bnb-chain/greenfield/types"
	"github.com/bnb-chain/greenfield/types/s3util"
	permTypes "github.com/bnb-chain/greenfield/x/permission/types"
	storageTypes "github.com/bnb-chain/greenfield/x/storage/types"
	sdk "github.com/cosmos/cosmos-sdk/types"
	"github.com/cosmos/cosmos-sdk/types/tx"
	"github.com/rs/zerolog/log"
)

type Object interface {
	GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error)
	CreateObject(ctx context.Context, bucketName, objectName string, reader io.Reader, opts types.CreateObjectOptions) (string, error)
	PutObject(ctx context.Context, bucketName, objectName string, objectSize int64, reader io.Reader, opts types.PutObjectOptions) error
	putObjectResumable(ctx context.Context, bucketName, objectName string, objectSize int64, reader io.Reader, opts types.PutObjectOptions) error
	FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error)
	CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error)
	DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error)
	GetObject(ctx context.Context, bucketName, objectName string, opts types.GetObjectOptions) (io.ReadCloser, types.ObjectStat, error)
	FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error
	FGetObjectResumable(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error

	// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
	// return err info if object not exist
	HeadObject(ctx context.Context, bucketName, objectName string) (*types.ObjectDetail, error)
	// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
	// return err info if object not exist
	HeadObjectByID(ctx context.Context, objID string) (*types.ObjectDetail, error)
	// UpdateObjectVisibility update the visibility of the object
	UpdateObjectVisibility(ctx context.Context, bucketName, objectName string, visibility storageTypes.VisibilityType, opt types.UpdateObjectOption) (string, error)
	// PutObjectPolicy apply object policy to the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	PutObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal,
		statements []*permTypes.Statement, opt types.PutPolicyOption) (string, error)
	// DeleteObjectPolicy delete the object policy of the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal, opt types.DeletePolicyOption) (string, error)
	// GetObjectPolicy get the object policy info of the user specified by principalAddr.
	// principalAddr indicates the HEX-encoded string of the principal address
	GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error)
	// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
	// userAddr indicates the HEX-encoded string of the user address
	IsObjectPermissionAllowed(ctx context.Context, userAddr string, bucketName, objectName string, action permTypes.ActionType) (permTypes.Effect, error)
	ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error)
	// ComputeHashRoots compute the integrity hash, content size and the redundancy type of the file
	// If isSerial is true, compute the integrity hash using the serial way
	// If isSerial is false or not provided, compute the integrity hash using the parallel way
	ComputeHashRoots(reader io.Reader, isSerial bool) ([][]byte, int64, storageTypes.RedundancyType, error)

	// CreateFolder creates an empty object used as folder.
	// objectName must ending with a forward slash (/) character
	CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error)
	// GetObjectUploadProgress return the status of the uploading object
	GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error)

	// GetObjectResumableUploadOffset return the status of the uploading object
	GetObjectResumableUploadOffset(ctx context.Context, bucketName, objectName string) (uint64, error)
	// ListObjectsByObjectID list objects by object ids
	ListObjectsByObjectID(ctx context.Context, objectIds []uint64, opts types.EndPointOptions) (types.ListObjectsByObjectIDResponse, error)
}

// GetRedundancyParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetRedundancyParams() (uint32, uint32, uint64, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return 0, 0, 0, err
	}

	versionedParams := queryResp.Params.VersionedParams
	return versionedParams.GetRedundantDataChunkNum(), versionedParams.GetRedundantParityChunkNum(), versionedParams.GetMaxSegmentSize(), nil
}

// GetParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetParams() (storageTypes.Params, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return storageTypes.Params{}, err
	}

	return queryResp.Params, nil
}

// ComputeHashRoots return the integrity hash, content size and the redundancy type of the file
func (c *client) ComputeHashRoots(reader io.Reader, isSerial bool) ([][]byte, int64, storageTypes.RedundancyType, error) {
	dataBlocks, parityBlocks, segSize, err := c.GetRedundancyParams()
	if reader == nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, errors.New("fail to compute hash, reader is nil")
	}
	if err != nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, err
	}

	return hashlib.ComputeIntegrityHash(reader, int64(segSize), int(dataBlocks), int(parityBlocks), isSerial)
}

// CreateObject get approval of creating object and send createObject txn to greenfield chain,
// it returns the transaction hash value and error
func (c *client) CreateObject(ctx context.Context, bucketName, objectName string,
	reader io.Reader, opts types.CreateObjectOptions,
) (string, error) {
	if reader == nil {
		return "", errors.New("fail to compute hash of payload, reader is nil")
	}

	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	// compute hash root of payload
	expectCheckSums, size, redundancyType, err := c.ComputeHashRoots(reader, opts.IsSerialComputeMode)
	if err != nil {
		return "", err
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	var visibility storageTypes.VisibilityType
	if opts.Visibility == storageTypes.VISIBILITY_TYPE_UNSPECIFIED {
		visibility = storageTypes.VISIBILITY_TYPE_INHERIT // set default visibility type
	} else {
		visibility = opts.Visibility
	}

	createObjectMsg := storageTypes.NewMsgCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName,
		uint64(size), visibility, expectCheckSums, contentType, redundancyType, math.MaxUint, nil)
	err = createObjectMsg.ValidateBasic()
	if err != nil {
		return "", err
	}

	signedCreateObjectMsg, err := c.GetCreateObjectApproval(ctx, createObjectMsg)
	if err != nil {
		return "", err
	}

	// set the default txn broadcast mode as block mode
	if opts.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opts.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	resp, err := c.chainClient.BroadcastTx(ctx, []sdk.Msg{signedCreateObjectMsg}, opts.TxOpts)
	if err != nil {
		return "", err
	}

	txnHash := resp.TxResponse.TxHash
	var txnResponse *ctypes.ResultTx
	if !opts.IsAsyncMode {
		ctxTimeout, cancel := context.WithTimeout(ctx, types.ContextTimeout)
		defer cancel()
		txnResponse, err = c.WaitForTx(ctxTimeout, txnHash)
		if err != nil {
			return txnHash, fmt.Errorf("the transaction has been submitted, please check it later:%v", err)
		}
		if txnResponse.TxResult.Code != 0 {
			return txnHash, fmt.Errorf("the createObject txn has failed with response code: %d", txnResponse.TxResult.Code)
		}
	}
	return txnHash, nil
}

// DeleteObject send DeleteBucket txn to greenfield chain and return txn hash
func (c *client) DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	delObjectMsg := storageTypes.NewMsgDeleteObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, delObjectMsg, opt.TxOpts)
}

// CancelCreateObject send CancelCreateObject txn to greenfield chain
func (c *client) CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	cancelCreateMsg := storageTypes.NewMsgCancelCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, cancelCreateMsg, opt.TxOpts)
}

// PutObject supports the second stage of uploading the object to bucket.
// txnHash should be the str which hex.encoding from txn hash bytes
func (c *client) PutObject(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if objectSize <= 0 {
		return errors.New("object size should be more than 0")
	}

	params, err := c.GetParams()
	if err != nil {
		return err
	}
	// minPartSize: 16MB
	if opts.PartSize == 0 {
		opts.PartSize = types.MinPartSize
	}
	if opts.PartSize%params.GetMaxSegmentSize() != 0 {
		return errors.New("part size should be an integer multiple of the segment size")
	}

	// upload an entire object to the storage provider in a single request
	if objectSize <= int64(opts.PartSize) || opts.DisableResumable {
		return c.putObject(ctx, bucketName, objectName, objectSize, reader, opts)
	}

	// resumableupload
	return c.putObjectResumable(ctx, bucketName, objectName, objectSize, reader, opts)
}

func (c *client) putObject(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if err := c.headSPObjectInfo(ctx, bucketName, objectName); err != nil {
		log.Error().Msg(fmt.Sprintf("fail to head object %s , err %v ", objectName, err))
		return err
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
		contentLength: objectSize,
		contentType:   contentType,
	}

	var sendOpt sendOptions
	if opts.TxnHash != "" {
		sendOpt = sendOptions{
			method:  http.MethodPut,
			body:    reader,
			txnHash: opts.TxnHash,
		}
	} else {
		sendOpt = sendOptions{
			method: http.MethodPut,
			body:   reader,
		}
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return err
	}

	_, err = c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return err
	}

	return nil
}

// UploadSegmentHook is for testing usage
type uploadSegmentHook func(id int) error

var UploadSegmentHooker uploadSegmentHook = DefaultUploadSegment

func DefaultUploadSegment(id int) error {
	return nil
}

func (c *client) putObjectResumable(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if err := c.headSPObjectInfo(ctx, bucketName, objectName); err != nil {
		return err
	}

	offset, err := c.GetObjectResumableUploadOffset(ctx, bucketName, objectName)
	if err != nil {
		return err
	}

	// Total data read and written to server. should be equal to
	// 'size' at the end of the call.
	var totalUploadedSize int64

	// Calculate the optimal parts info for a given size.
	totalPartsCount, partSize, _, err := c.SplitPartInfo(objectSize, opts.PartSize)
	if err != nil {
		return err
	}

	// Part number always starts with '1'.
	partNumber := 1
	startPartNumber := int(offset/opts.PartSize + 1)

	// Create a buffer.
	buf := make([]byte, partSize)
	complete := false

	//  TODO(chris): Skip successful segments or add a verification file check.
	for partNumber < startPartNumber {
		length, rErr := utils.ReadFull(reader, buf)
		if rErr == io.EOF && partNumber > 1 {
			break
		}
		// Increment part number.
		log.Debug().Msg(fmt.Sprintf("skip partNumber:%d, length:%d", partNumber, length))
		// Save successfully uploaded size.
		totalUploadedSize += int64(length)
		partNumber++
	}

	for partNumber <= totalPartsCount {
		if partNumber == totalPartsCount {
			complete = true
		}
		if err = UploadSegmentHooker(partNumber); err != nil {
			return err
		}
		length, rErr := utils.ReadFull(reader, buf)
		if rErr == io.EOF && partNumber > 1 {
			break
		}

		if rErr != nil && rErr != io.ErrUnexpectedEOF && rErr != io.EOF {
			return err
		}

		log.Debug().Msg(fmt.Sprintf("partNumber:%d, length:%d", partNumber, length))

		// Update progress reader appropriately to the latest offset
		// as we read from the source.
		rd := bytes.NewReader(buf[:length])

		var contentType string
		if opts.ContentType != "" {
			contentType = opts.ContentType
		} else {
			contentType = types.ContentDefault
		}

		// Initialize url queries.
		urlValues := make(url.Values)
		urlValues.Set("offset", strconv.FormatInt(totalUploadedSize, 10))
		urlValues.Set("complete", strconv.FormatBool(complete))

		reqMeta := requestMeta{
			bucketName:    bucketName,
			objectName:    objectName,
			contentLength: int64(length),
			contentType:   contentType,
			urlValues:     urlValues,
		}

		var sendOpt sendOptions
		if opts.TxnHash != "" {
			sendOpt = sendOptions{
				method:  http.MethodPost,
				body:    rd,
				txnHash: opts.TxnHash,
			}
		} else {
			sendOpt = sendOptions{
				method: http.MethodPost,
				body:   rd,
			}
		}

		endpoint, err := c.getSPUrlByBucket(bucketName)
		if err != nil {
			log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
			return err
		}

		// Proceed to upload the part.
		_, err = c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
		if err != nil {
			return err
		}

		// Save successfully uploaded size.
		totalUploadedSize += int64(length)

		// Increment part number.
		partNumber++

		// For unknown size, Read EOF we break away.
		// We do not have to upload till totalPartsCount.
		if rErr == io.EOF {
			break
		}
	}

	return nil
}

func (c *client) headSPObjectInfo(ctx context.Context, bucketName, objectName string) error {
	backoffDelay := types.HeadBackOffDelay
	for retry := 0; retry < types.MaxHeadTryTime; retry++ {
		_, err := c.getObjectStatusFromSP(ctx, bucketName, objectName)
		if err == nil {
			return nil
		}
		// if the error is not "no such object", ignore it
		if !strings.Contains(strings.ToLower(err.Error()), types.NoSuchObjectErr) {
			return nil
		}

		if retry == types.MaxHeadTryTime-1 {
			return fmt.Errorf(" sp failed to head info of the object: %s, please try putObject later", objectName)
		}

		time.Sleep(backoffDelay)
		backoffDelay *= 2
	}

	return nil
}

// FPutObject supports uploading object from local file
func (c *client) FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error) {
	fReader, err := os.Open(filePath)
	// If any error fail quickly here.
	if err != nil {
		return err
	}
	defer fReader.Close()

	// Save the file stat.
	stat, err := fReader.Stat()
	if err != nil {
		return err
	}

	return c.PutObject(ctx, bucketName, objectName, stat.Size(), fReader, opts)
}

// GetObject download s3 object payload and return the related object info
func (c *client) GetObject(ctx context.Context, bucketName, objectName string,
	opts types.GetObjectOptions,
) (io.ReadCloser, types.ObjectStat, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	if opts.Range != "" {
		reqMeta.rangeInfo = opts.Range
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed,  err: %s", bucketName, err.Error()))
		return nil, types.ObjectStat{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, types.ObjectStat{}, err
	}

	objStat, err := getObjInfo(objectName, resp.Header)
	if err != nil {
		utils.CloseResponse(resp)
		return nil, types.ObjectStat{}, err
	}

	return resp.Body, objStat, nil
}

// FGetObject download s3 object payload adn write the object content into local file specified by filePath
func (c *client) FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error {
	// Verify if destination already exists.
	st, err := os.Stat(filePath)
	if err == nil {
		// If the destination exists and is a directory.
		if st.IsDir() {
			return errors.New("download file path is a directory")
		}
		return errors.New("download file already exist")
	}

	fd, err := os.OpenFile(filePath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o660)
	if err != nil {
		return err
	}

	body, _, err := c.GetObject(ctx, bucketName, objectName, opts)
	if err != nil {
		return err
	}
	defer body.Close()

	_, err = io.Copy(fd, body)
	fd.Close()
	if err != nil {
		return err
	}

	return nil
}

// GetSegmentEnd calculates the end position
func GetSegmentEnd(begin int64, total int64, per int64) int64 {
	if begin+per > total {
		return total - 1
	}
	return begin + per - 1
}

// FGetObjectResumable download s3 object payload with resumable download
func (c *client) FGetObjectResumable(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOptions) error {
	// Get the object detailed meta for object whole size
	meta, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return err
	}

	tempFilePath := filePath + "_" + c.defaultAccount.GetAddress().String() + opts.Range + types.TempFileSuffix

	var (
		startOffset    int64
		endOffset      int64
		partEndOffset  int64
		maxSegmentSize int64
		objectOption   types.GetObjectOptions
		segNum         int64
		partSize       int64
	)

	// 1) check paramter
	params, err := c.GetParams()
	if err != nil {
		return err
	}
	maxSegmentSize = int64(params.GetMaxSegmentSize())

	// minPartSize: 16MB
	if opts.PartSize == 0 {
		partSize = types.MinPartSize
	} else {
		partSize = int64(opts.PartSize)
	}

	if partSize%maxSegmentSize != 0 {
		return errors.New("part size should be an integer multiple of the segment size")
	}

	isRange, rangeStart, rangeEnd := utils.ParseRange(opts.Range)
	if isRange && (rangeEnd < 0 || rangeEnd >= int64(meta.ObjectInfo.GetPayloadSize())) {
		rangeEnd = int64(meta.ObjectInfo.GetPayloadSize()) - 1
	}

	if isRange {
		startOffset = rangeStart
		endOffset = rangeEnd
	} else {
		startOffset = 0
		endOffset = int64(meta.ObjectInfo.GetPayloadSize()) - 1
	}

	// 2)prepare and check temp file
	fileInfo, err := os.Stat(tempFilePath)
	if err != nil {
		if !strings.Contains(err.Error(), "no such file or directory") {
			return err
		}
	} else {
		/*
				| -----seg1 ------ | -----seg2 ------ | -----seg3 ------ | -----seg4 ------ |
				        ^                                                         ^
				        |													      |
				 startOffset													  endOffset
			            |----------|
		*/
		fileSize := fileInfo.Size()
		var (
			firstSegSize   int64
			file           *os.File
			truncateOffset int64
		)

		if isRange {
			firstSegSize = partSize - rangeStart%partSize
		} else {
			firstSegSize = partSize
		}
		fileSizeWithoutFirstSeg := fileSize - firstSegSize
		if fileSize > firstSegSize {
			truncateOffset = ((fileSize-firstSegSize)/partSize)*partSize + firstSegSize
			startOffset = ((fileSize-firstSegSize)/partSize)*partSize + partSize
		} else {
			truncateOffset = 0
			startOffset = 0
		}
		log.Debug().Msgf("The file:%s size:%d, startOffset:%d, range:%s\n", tempFilePath, fileSize, startOffset, opts.Range)

		// truncated file to part size integer multiples
		if fileSizeWithoutFirstSeg%partSize != 0 {
			file, err = os.OpenFile(tempFilePath, os.O_RDWR, 0644)
			if err != nil {
				return err
			}
			defer file.Close()

			err = file.Truncate(truncateOffset)
			if err != nil {
				return err
			}
			log.Debug().Msgf("The file was truncated to the specified size.%d\n", truncateOffset)
			// TODO(chris): verify file's segment
		}
	}

	// Create the file if not exists. Otherwise the segments download will overwrite it.
	fd, err := os.OpenFile(tempFilePath, os.O_WRONLY|os.O_CREATE|os.O_APPEND, types.FilePermMode)
	if err != nil {
		return err
	}
	_, err = fd.Seek(startOffset, io.SeekStart)
	if err != nil {
		fd.Close()
		return err
	}

	log.Debug().Msg(fmt.Sprintf("get object resumeable begin segment Range: %s, startOffset: %d, endOffset:%d", opts.Range, startOffset, endOffset))

	// 3) Downloading Parts Sequentially based on partSize
	segNum = startOffset / partSize
	for partStartOffset := startOffset; partStartOffset < endOffset; partStartOffset += partSize {
		// hook for test
		if err = DownloadSegmentHooker(segNum); err != nil {
			return err
		}

		partEndOffset = GetSegmentEnd(partStartOffset, endOffset+1, partSize)
		err = objectOption.SetRange(partStartOffset, partEndOffset)
		if err != nil {
			return err
		}

		startT := time.Now().UnixNano() / 1000 / 1000 / 1000

		rd, _, err := c.GetObject(ctx, bucketName, objectName, objectOption)
		if err != nil {
			return err
		}
		defer rd.Close()

		_, err = io.Copy(fd, rd)
		log.Debug().Msg(fmt.Sprintf("get object for segment Range: %s, current partStartOffset: %d, segNum: %d", objectOption.Range, partStartOffset, segNum))
		endT := time.Now().UnixNano() / 1000 / 1000 / 1000
		if err != nil {
			log.Error().Msg(fmt.Sprintf("get seg error,cost:%d second,seg number:%d,error:%s.\n", endT-startT, segNum, err.Error()))
			fd.Close()
		}

		segNum++
	}

	fd.Close()

	// 4) rename temp file
	err = os.Rename(tempFilePath, filePath)
	if err != nil {
		return err
	}

	return nil
}

// getObjInfo generates objectInfo base on the response http header content
func getObjInfo(objectName string, h http.Header) (types.ObjectStat, error) {
	// Parse content length is exists
	var size int64 = -1
	var err error
	contentLength := h.Get(types.HTTPHeaderContentLength)
	if contentLength != "" {
		size, err = strconv.ParseInt(contentLength, 10, 64)
		if err != nil {
			return types.ObjectStat{}, types.ErrResponse{
				Code:    "InternalError",
				Message: fmt.Sprintf("Content-Length parse error %v", err),
			}
		}
	}

	// fetch content type
	contentType := strings.TrimSpace(h.Get("Content-Type"))
	if contentType == "" {
		contentType = types.ContentDefault
	}

	return types.ObjectStat{
		ObjectName:  objectName,
		ContentType: contentType,
		Size:        size,
	}, nil
}

// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObject(ctx context.Context, bucketName, objectName string) (*types.ObjectDetail, error) {
	queryHeadObjectRequest := storageTypes.QueryHeadObjectRequest{
		BucketName: bucketName,
		ObjectName: objectName,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObject(ctx, &queryHeadObjectRequest)
	if err != nil {
		return nil, err
	}

	return &types.ObjectDetail{
		ObjectInfo:         queryHeadObjectResponse.ObjectInfo,
		GlobalVirtualGroup: queryHeadObjectResponse.GlobalVirtualGroup,
	}, nil
}

// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObjectByID(ctx context.Context, objID string) (*types.ObjectDetail, error) {
	headObjectRequest := storageTypes.QueryHeadObjectByIdRequest{
		ObjectId: objID,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObjectById(ctx, &headObjectRequest)
	if err != nil {
		return nil, err
	}

	return &types.ObjectDetail{
		ObjectInfo:         queryHeadObjectResponse.ObjectInfo,
		GlobalVirtualGroup: queryHeadObjectResponse.GlobalVirtualGroup,
	}, nil
}

// PutObjectPolicy apply object policy to the principal, return the txn hash
func (c *client) PutObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal,
	statements []*permTypes.Statement, opt types.PutPolicyOption,
) (string, error) {
	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)

	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	putPolicyMsg := storageTypes.NewMsgPutPolicy(c.MustGetDefaultAccount().GetAddress(), resource.String(),
		principal, statements, opt.PolicyExpireTime)

	return c.sendPutPolicyTxn(ctx, putPolicyMsg, opt.TxOpts)
}

// DeleteObjectPolicy delete the object policy of the principal
func (c *client) DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal, opt types.DeletePolicyOption) (string, error) {
	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	return c.sendDelPolicyTxn(ctx, c.MustGetDefaultAccount().GetAddress(), resource.String(), principal, opt.TxOpts)
}

// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
func (c *client) IsObjectPermissionAllowed(ctx context.Context, userAddr string,
	bucketName, objectName string, action permTypes.ActionType,
) (permTypes.Effect, error) {
	_, err := sdk.AccAddressFromHexUnsafe(userAddr)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}
	verifyReq := storageTypes.QueryVerifyPermissionRequest{
		Operator:   userAddr,
		BucketName: bucketName,
		ObjectName: objectName,
		ActionType: action,
	}

	verifyResp, err := c.chainClient.VerifyPermission(ctx, &verifyReq)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}

	return verifyResp.Effect, nil
}

// GetObjectPolicy get the object policy info of the user specified by principalAddr
func (c *client) GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error) {
	_, err := sdk.AccAddressFromHexUnsafe(principalAddr)
	if err != nil {
		return nil, err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	queryPolicy := storageTypes.QueryPolicyForAccountRequest{
		Resource:         resource.String(),
		PrincipalAddress: principalAddr,
	}

	queryPolicyResp, err := c.chainClient.QueryPolicyForAccount(ctx, &queryPolicy)
	if err != nil {
		return nil, err
	}

	return queryPolicyResp.Policy, nil
}

// ListObjects return object list of the specific bucket
func (c *client) ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return types.ListObjectsResult{}, err
	}

	const listObjectsDefaultMaxKeys = 1000
	if opts.MaxKeys == 0 {
		opts.MaxKeys = listObjectsDefaultMaxKeys
	}

	if opts.StartAfter != "" {
		if err := s3util.CheckValidObjectName(opts.StartAfter); err != nil {
			return types.ListObjectsResult{}, err
		}
	}

	if opts.ContinuationToken != "" {
		decodedContinuationToken, err := base64.StdEncoding.DecodeString(opts.ContinuationToken)
		if err != nil {
			return types.ListObjectsResult{}, err
		}
		objectName := string(decodedContinuationToken)
		if err = s3util.CheckValidObjectName(objectName); err != nil {
			return types.ListObjectsResult{}, err
		}
		if !strings.HasPrefix(objectName, opts.Prefix) {
			return types.ListObjectsResult{}, fmt.Errorf("continuation-token does not match the input prefix")
		}
	}

	if ok := utils.IsValidObjectPrefix(opts.Prefix); !ok {
		return types.ListObjectsResult{}, fmt.Errorf("invalid object prefix")
	}

	params := url.Values{}
	params.Set("max-keys", strconv.FormatUint(opts.MaxKeys, 10))
	params.Set("start-after", opts.StartAfter)
	params.Set("continuation-token", opts.ContinuationToken)
	params.Set("delimiter", opts.Delimiter)
	params.Set("prefix", opts.Prefix)
	params.Set("include-removed", strconv.FormatBool(opts.ShowRemovedObject))
	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getEndpointByOpt(opts.EndPointOptions)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("get endpoint by option failed %s", err.Error()))
		return types.ListObjectsResult{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.ListObjectsResult{}, err
	}
	defer utils.CloseResponse(resp)

	// unmarshal the json content from response body
	buf := new(strings.Builder)
	_, err = io.Copy(buf, resp.Body)
	if err != nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	listObjectsResult := types.ListObjectsResult{}
	bufStr := buf.String()
	err = json.Unmarshal([]byte(bufStr), &listObjectsResult)
	// TODO(annie) remove tolerance for unmarshal err after structs got stabilized
	if err != nil && listObjectsResult.Objects == nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	if opts.ShowRemovedObject {
		return listObjectsResult, nil
	}

	// default only return the object that has not been removed
	objectMetaList := make([]*types.ObjectMeta, 0)
	for _, objectInfo := range listObjectsResult.Objects {
		if objectInfo.Removed {
			continue
		}

		objectMetaList = append(objectMetaList, objectInfo)
	}

	listObjectsResult.Objects = objectMetaList
	listObjectsResult.KeyCount = strconv.Itoa(len(objectMetaList))
	return listObjectsResult, nil
}

// GetCreateObjectApproval returns the signature info for the approval of preCreating resources
func (c *client) GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error) {
	unsignedBytes := createObjectMsg.GetSignBytes()

	// set the action type
	urlValues := url.Values{
		"action": {types.CreateObjectAction},
	}

	reqMeta := requestMeta{
		urlValues:     urlValues,
		urlRelPath:    "get-approval",
		contentSHA256: types.EmptyStringSHA256,
		txnMsg:        hex.EncodeToString(unsignedBytes),
	}

	sendOpt := sendOptions{
		method:     http.MethodGet,
		isAdminApi: true,
	}

	bucketName := createObjectMsg.BucketName
	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return nil, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, err
	}

	// fetch primary signed msg from sp response
	signedRawMsg := resp.Header.Get(types.HTTPHeaderSignedMsg)
	if signedRawMsg == "" {
		return nil, errors.New("fail to fetch pre createObject signature")
	}

	signedMsgBytes, err := hex.DecodeString(signedRawMsg)
	if err != nil {
		return nil, err
	}

	var signedMsg storageTypes.MsgCreateObject
	storageTypes.ModuleCdc.MustUnmarshalJSON(signedMsgBytes, &signedMsg)

	return &signedMsg, nil
}

// CreateFolder send create empty object txn to greenfield chain
func (c *client) CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error) {
	if !strings.HasSuffix(objectName, "/") {
		return "", errors.New("failed to create folder. Folder names must end with a forward slash (/) character")
	}

	reader := bytes.NewReader([]byte(``))
	txHash, err := c.CreateObject(ctx, bucketName, objectName, reader, opts)
	return txHash, err
}

// GetObjectUploadProgress return the status of object including the uploading progress
func (c *client) GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error) {
	status, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", err
	}

	// get object status from sp
	if status.ObjectInfo.ObjectStatus == storageTypes.OBJECT_STATUS_CREATED {
		uploadProgressInfo, err := c.getObjectStatusFromSP(ctx, bucketName, objectName)
		if err != nil {
			return "", errors.New("fail to fetch object uploading progress from sp" + err.Error())
		}
		return uploadProgressInfo.ProgressDescription, nil
	}

	return status.ObjectInfo.ObjectStatus.String(), nil
}

// GetObjectResumableUploadOffset return the status of object including the uploading progress
func (c *client) GetObjectResumableUploadOffset(ctx context.Context, bucketName, objectName string) (uint64, error) {
	status, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return 0, err
	}

	// get object status from sp
	if status.ObjectInfo.ObjectStatus == storageTypes.OBJECT_STATUS_CREATED {
		uploadOffsetInfo, err := c.getObjectOffsetFromSP(ctx, bucketName, objectName)
		if err != nil {
			return 0, errors.New("fail to fetch object uploading offset from sp" + err.Error())
		}
		log.Debug().Msgf("get object resumable upload offset %d from sp", uploadOffsetInfo.Offset)
		return uploadOffsetInfo.Offset, nil
	}

	// TODO(chris): may error
	return 0, nil
}

func (c *client) getObjectOffsetFromSP(ctx context.Context, bucketName, objectName string) (types.UploadOffset, error) {
	params := url.Values{}
	params.Set("upload-context", "")

	reqMeta := requestMeta{
		urlValues:  params,
		bucketName: bucketName,
		objectName: objectName,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		return types.UploadOffset{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)

	if err != nil {
		// not exist
		if find := strings.Contains(err.Error(), "no uploading record"); find {
			return types.UploadOffset{Offset: 0}, nil
		} else {
			return types.UploadOffset{}, err
		}
	}

	defer utils.CloseResponse(resp)

	objectOffset := types.UploadOffset{}
	// decode the xml content from response body
	err = xml.NewDecoder(resp.Body).Decode(&objectOffset)
	if err != nil {
		return types.UploadOffset{}, err
	}

	return objectOffset, nil
}

func (c *client) getObjectStatusFromSP(ctx context.Context, bucketName, objectName string) (types.UploadProgress, error) {
	params := url.Values{}
	params.Set("upload-progress", "")

	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		return types.UploadProgress{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.UploadProgress{}, err
	}

	defer utils.CloseResponse(resp)

	objectStatus := types.UploadProgress{}
	// decode the xml content from response body
	err = xml.NewDecoder(resp.Body).Decode(&objectStatus)
	if err != nil {
		return types.UploadProgress{}, err
	}

	return objectStatus, nil
}

func (c *client) UpdateObjectVisibility(ctx context.Context, bucketName, objectName string,
	visibility storageTypes.VisibilityType, opt types.UpdateObjectOption,
) (string, error) {
	object, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", fmt.Errorf("object:%s not exists: %s\n", objectName, err.Error())
	}

	if object.ObjectInfo.GetVisibility() == visibility {
		return "", fmt.Errorf("the visibility of object:%s is already %s \n", objectName, visibility.String())
	}

	updateObjectMsg := storageTypes.NewMsgUpdateObjectInfo(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName, visibility)

	// set the default txn broadcast mode as sync mode
	if opt.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opt.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	return c.sendTxn(ctx, updateObjectMsg, opt.TxOpts)
}

// ListObjectsByObjectID list objects by object ids
// By inputting a collection of object IDs, we can retrieve the corresponding object data.
// If the object is nonexistent or has been deleted, a null value will be returned
func (c *client) ListObjectsByObjectID(ctx context.Context, objectIds []uint64, opts types.EndPointOptions) (types.ListObjectsByObjectIDResponse, error) {
	const MaximumListObjectsSize = 1000
	if len(objectIds) == 0 || len(objectIds) > MaximumListObjectsSize {
		return types.ListObjectsByObjectIDResponse{}, nil
	}

	objectIDMap := make(map[uint64]bool)
	for _, id := range objectIds {
		if _, ok := objectIDMap[id]; ok {
			// repeat id keys in request
			return types.ListObjectsByObjectIDResponse{}, nil
		}
		objectIDMap[id] = true
	}

	params := url.Values{}
	params.Set("objects-query", "")

	reqMeta := requestMeta{
		urlValues:     params,
		contentSHA256: types.EmptyStringSHA256,
	}

	b, err := json.Marshal(types.ObjectAndBucketIDs{IDs: objectIds})
	if err != nil {
		return types.ListObjectsByObjectIDResponse{}, err
	}

	sendOpt := sendOptions{
		method:           http.MethodPost,
		body:             bytes.NewBuffer(b),
		disableCloseBody: true,
	}

	endpoint, err := c.getEndpointByOpt(&opts)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("get endpoint by option failed %s", err.Error()))
		return types.ListObjectsByObjectIDResponse{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.ListObjectsByObjectIDResponse{}, err
	}
	defer utils.CloseResponse(resp)

	// unmarshal the json content from response body
	buf := new(strings.Builder)
	_, err = io.Copy(buf, resp.Body)
	if err != nil {
		log.Error().Msgf("the list of objects in object ids:%v failed: %s", objectIds, err.Error())
		return types.ListObjectsByObjectIDResponse{}, err
	}

	objects := types.ListObjectsByObjectIDResponse{}
	bufStr := buf.String()
	err = json.Unmarshal([]byte(bufStr), &objects)
	if err != nil && objects.Objects == nil {
		log.Error().Msgf("the list of objects in object ids:%v failed: %s", objectIds, err.Error())
		return types.ListObjectsByObjectIDResponse{}, err
	}

	return objects, nil
}

package client

import (
	"bytes"
	"context"
	"encoding/base64"
	"encoding/hex"
	"encoding/json"
	"encoding/xml"
	"errors"
	"fmt"
	"io"
	"math"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"strings"
	"time"

	hashlib "github.com/bnb-chain/greenfield-common/go/hash"
	gnfdsdk "github.com/bnb-chain/greenfield/sdk/types"
	gnfdTypes "github.com/bnb-chain/greenfield/types"
	"github.com/bnb-chain/greenfield/types/s3util"
	permTypes "github.com/bnb-chain/greenfield/x/permission/types"
	storageTypes "github.com/bnb-chain/greenfield/x/storage/types"
	sdk "github.com/cosmos/cosmos-sdk/types"
	"github.com/cosmos/cosmos-sdk/types/tx"
	"github.com/rs/zerolog/log"

	"github.com/bnb-chain/greenfield-go-sdk/pkg/utils"
	"github.com/bnb-chain/greenfield-go-sdk/types"
)

type Object interface {
	GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error)
	CreateObject(ctx context.Context, bucketName, objectName string, reader io.Reader, opts types.CreateObjectOptions) (string, error)
	PutObject(ctx context.Context, bucketName, objectName string, objectSize int64, reader io.Reader, opts types.PutObjectOptions) error
	FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error)
	CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error)
	DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error)
	GetObject(ctx context.Context, bucketName, objectName string, opts types.GetObjectOption) (io.ReadCloser, types.ObjectStat, error)
	FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOption) error
	GetObjectResumable(ctx context.Context, bucketName, objectName string, opts types.GetObjectResumableOption, filePath string) error

	// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
	// return err info if object not exist
	HeadObject(ctx context.Context, bucketName, objectName string) (*storageTypes.ObjectInfo, error)
	// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
	// return err info if object not exist
	HeadObjectByID(ctx context.Context, objID string) (*storageTypes.ObjectInfo, error)
	// UpdateObjectVisibility update the visibility of the object
	UpdateObjectVisibility(ctx context.Context, bucketName, objectName string, visibility storageTypes.VisibilityType, opt types.UpdateObjectOption) (string, error)
	// PutObjectPolicy apply object policy to the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	PutObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal,
		statements []*permTypes.Statement, opt types.PutPolicyOption) (string, error)
	// DeleteObjectPolicy delete the object policy of the principal, return the txn hash
	// The principal can be generated by NewPrincipalWithAccount or NewPrincipalWithGroupId
	DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principal types.Principal, opt types.DeletePolicyOption) (string, error)
	// GetObjectPolicy get the object policy info of the user specified by principalAddr.
	// principalAddr indicates the HEX-encoded string of the principal address
	GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error)
	// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
	// userAddr indicates the HEX-encoded string of the user address
	IsObjectPermissionAllowed(ctx context.Context, userAddr string, bucketName, objectName string, action permTypes.ActionType) (permTypes.Effect, error)
	ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error)
	// ComputeHashRoots compute the integrity hash, content size and the redundancy type of the file
	ComputeHashRoots(reader io.Reader) ([][]byte, int64, storageTypes.RedundancyType, error)

	// CreateFolder creates an empty object used as folder.
	// objectName must ending with a forward slash (/) character
	CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error)

	// GetObjectUploadProgress return the status of the uploading object
	GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error)
}

// GetRedundancyParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetRedundancyParams() (uint32, uint32, uint64, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return 0, 0, 0, err
	}

	versionedParams := queryResp.Params.VersionedParams
	return versionedParams.GetRedundantDataChunkNum(), versionedParams.GetRedundantParityChunkNum(), versionedParams.GetMaxSegmentSize(), nil
}

// GetParams query and return the data shards, parity shards and segment size of redundancy
// configuration on chain
func (c *client) GetParams() (storageTypes.Params, error) {
	query := storageTypes.QueryParamsRequest{}
	queryResp, err := c.chainClient.StorageQueryClient.Params(context.Background(), &query)
	if err != nil {
		return storageTypes.Params{}, err
	}

	return queryResp.Params, nil
}

// ComputeHashRoots return the integrity hash, content size and the redundancy type of the file
func (c *client) ComputeHashRoots(reader io.Reader) ([][]byte, int64, storageTypes.RedundancyType, error) {
	dataBlocks, parityBlocks, segSize, err := c.GetRedundancyParams()
	if reader == nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, errors.New("fail to compute hash, reader is nil")
	}
	if err != nil {
		return nil, 0, storageTypes.REDUNDANCY_EC_TYPE, err
	}

	return hashlib.ComputeIntegrityHash(reader, int64(segSize), int(dataBlocks), int(parityBlocks))
}

// CreateObject get approval of creating object and send createObject txn to greenfield chain,
// it returns the transaction hash value and error
func (c *client) CreateObject(ctx context.Context, bucketName, objectName string,
	reader io.Reader, opts types.CreateObjectOptions,
) (string, error) {
	if reader == nil {
		return "", errors.New("fail to compute hash of payload, reader is nil")
	}

	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	// compute hash root of payload
	expectCheckSums, size, redundancyType, err := c.ComputeHashRoots(reader)
	if err != nil {
		return "", err
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	var visibility storageTypes.VisibilityType
	if opts.Visibility == storageTypes.VISIBILITY_TYPE_UNSPECIFIED {
		visibility = storageTypes.VISIBILITY_TYPE_INHERIT // set default visibility type
	} else {
		visibility = opts.Visibility
	}

	createObjectMsg := storageTypes.NewMsgCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName,
		uint64(size), visibility, expectCheckSums, contentType, redundancyType, math.MaxUint, nil, opts.SecondarySPAccs)
	err = createObjectMsg.ValidateBasic()
	if err != nil {
		return "", err
	}

	signedCreateObjectMsg, err := c.GetCreateObjectApproval(ctx, createObjectMsg)
	if err != nil {
		return "", err
	}

	// set the default txn broadcast mode as block mode
	if opts.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opts.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	resp, err := c.chainClient.BroadcastTx(ctx, []sdk.Msg{signedCreateObjectMsg}, opts.TxOpts)
	if err != nil {
		return "", err
	}

	txnHash := resp.TxResponse.TxHash
	var txnResponse *sdk.TxResponse
	if !opts.IsAsyncMode {
		ctxTimeout, cancel := context.WithTimeout(ctx, types.ContextTimeout)
		defer cancel()
		txnResponse, err = c.WaitForTx(ctxTimeout, txnHash)
		if err != nil {
			return txnHash, fmt.Errorf("the transaction has been submitted, please check it later:%v", err)
		}
		if txnResponse.Code != 0 {
			return txnHash, fmt.Errorf("the createObject txn has failed with response code: %d", txnResponse.Code)
		}
	}
	return txnHash, nil
}

// DeleteObject send DeleteBucket txn to greenfield chain and return txn hash
func (c *client) DeleteObject(ctx context.Context, bucketName, objectName string, opt types.DeleteObjectOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	delObjectMsg := storageTypes.NewMsgDeleteObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, delObjectMsg, opt.TxOpts)
}

// CancelCreateObject send CancelCreateObject txn to greenfield chain
func (c *client) CancelCreateObject(ctx context.Context, bucketName, objectName string, opt types.CancelCreateOption) (string, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return "", err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return "", err
	}

	cancelCreateMsg := storageTypes.NewMsgCancelCreateObject(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName)
	return c.sendTxn(ctx, cancelCreateMsg, opt.TxOpts)
}

// PutObject supports the second stage of uploading the object to bucket.
// txnHash should be the str which hex.encoding from txn hash bytes
func (c *client) PutObject(ctx context.Context, bucketName, objectName string, objectSize int64,
	reader io.Reader, opts types.PutObjectOptions,
) (err error) {
	if objectSize <= 0 {
		return errors.New("object size should be more than 0")
	}

	var contentType string
	if opts.ContentType != "" {
		contentType = opts.ContentType
	} else {
		contentType = types.ContentDefault
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
		contentLength: objectSize,
		contentType:   contentType,
	}

	var sendOpt sendOptions
	if opts.TxnHash != "" {
		sendOpt = sendOptions{
			method:  http.MethodPut,
			body:    reader,
			txnHash: opts.TxnHash,
		}
	} else {
		sendOpt = sendOptions{
			method: http.MethodPut,
			body:   reader,
		}
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return err
	}

	_, err = c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return err
	}

	return nil
}

// FPutObject supports uploading object from local file
func (c *client) FPutObject(ctx context.Context, bucketName, objectName, filePath string, opts types.PutObjectOptions) (err error) {
	fReader, err := os.Open(filePath)
	// If any error fail quickly here.
	if err != nil {
		return err
	}
	defer fReader.Close()

	// Save the file stat.
	stat, err := fReader.Stat()
	if err != nil {
		return err
	}

	return c.PutObject(ctx, bucketName, objectName, stat.Size(), fReader, opts)
}

// GetObject download s3 object payload and return the related object info
func (c *client) GetObject(ctx context.Context, bucketName, objectName string,
	opts types.GetObjectOption,
) (io.ReadCloser, types.ObjectStat, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	if err := s3util.CheckValidObjectName(objectName); err != nil {
		return nil, types.ObjectStat{}, err
	}

	reqMeta := requestMeta{
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	if opts.Range != "" {
		reqMeta.rangeInfo = opts.Range
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed,  err: %s", bucketName, err.Error()))
		return nil, types.ObjectStat{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, types.ObjectStat{}, err
	}

	objStat, err := getObjInfo(objectName, resp.Header)
	if err != nil {
		utils.CloseResponse(resp)
		return nil, types.ObjectStat{}, err
	}

	return resp.Body, objStat, nil
}

// GetObjectResumable download s3 object payload and return the related object info
func (c *client) GetObjectResumable(ctx context.Context, bucketName, objectName string,
	opts types.GetObjectResumableOption, filePath string,
) error {
	tempFilePath := filePath + types.TempFileSuffix

	params, err := c.GetParams()
	if err != nil {
		return err
	}
	cpFilePath := types.GetDownloadCpFilePath(&opts.CpConfig, bucketName, objectName, "0", filePath)

	// Load checkpoint data.
	dcp := types.GetObjectCheckpoint{}
	err = dcp.Load(cpFilePath)
	if err != nil {
		os.Remove(cpFilePath)
	}

	// Get the object detailed meta for object whole size
	// must delete header:range to get whole object size
	meta, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return err
	}

	// Load error or data invalid. Re-initialize the download.
	// TODO(chris): uRange
	valid, err := dcp.IsValid(meta, nil)
	if err != nil || !valid {
		if err = dcp.Prepare(meta, objectName, filePath, int64(params.VersionedParams.GetMaxSegmentSize()), nil); err != nil {
			return err
		}
		os.Remove(cpFilePath)
	}

	// Create the file if not exists. Otherwise the parts download will overwrite it.
	fd, err := os.OpenFile(tempFilePath, os.O_WRONLY|os.O_CREATE, types.FilePermMode)
	if err != nil {
		return err
	}
	fd.Close()

	// Unfinished parts
	parts := dcp.TodoSegments()
	jobs := make(chan types.SegmentPiece, len(parts))
	results := make(chan types.SegmentPiece, len(parts))
	failed := make(chan error)
	die := make(chan bool)

	completedBytes := dcp.GetCompletedBytes()
	// TODO(chris): event

	// Start the download workers routine
	arg := DownloadWorkerArg{Client: c, BucketName: bucketName, ObjectName: objectName, FilePath: tempFilePath, hook: DownloadSegmentHooker, Ctx: ctx, Options: &opts}
	for w := 1; w <= 1; w++ {
		go DownloadWorker(w, arg, jobs, results, failed, die)
	}

	// Concurrently downloads parts
	go DownloadScheduler(jobs, parts)

	// Wait for the parts download finished
	completed := 0
	for completed < len(parts) {
		select {
		case seg := <-results:
			completed++
			dcp.SegmentStat[seg.Index] = true
			dcp.Segments[seg.Index].CRC64 = seg.CRC64
			err := dcp.Dump(cpFilePath)
			if err != nil {
				return err
			}
			downBytes := seg.End - seg.Start + 1
			completedBytes += downBytes
		case err := <-failed:
			close(die)
			return err
		}

		if completed >= len(parts) {
			break
		}
	}

	// TODO(chris) CRC
	//if dcp.EnableCRC {
	//}

	return dcp.Complete(cpFilePath, tempFilePath)
}

// FGetObject download s3 object payload adn write the object content into local file specified by filePath
func (c *client) FGetObject(ctx context.Context, bucketName, objectName, filePath string, opts types.GetObjectOption) error {
	// Verify if destination already exists.
	st, err := os.Stat(filePath)
	if err == nil {
		// If the destination exists and is a directory.
		if st.IsDir() {
			return errors.New("fileName is a directory.")
		}
	}

	// If file exist, open it in append mode
	fd, err := os.OpenFile(filePath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0o660)
	if err != nil {
		return err
	}

	body, _, err := c.GetObject(ctx, bucketName, objectName, opts)
	if err != nil {
		return err
	}
	defer body.Close()

	_, err = io.Copy(fd, body)
	fd.Close()
	if err != nil {
		return err
	}

	return nil
}

// getObjInfo generates objectInfo base on the response http header content
func getObjInfo(objectName string, h http.Header) (types.ObjectStat, error) {
	// Parse content length is exists
	var size int64 = -1
	var err error
	contentLength := h.Get(types.HTTPHeaderContentLength)
	if contentLength != "" {
		size, err = strconv.ParseInt(contentLength, 10, 64)
		if err != nil {
			return types.ObjectStat{}, types.ErrResponse{
				Code:    "InternalError",
				Message: fmt.Sprintf("Content-Length parse error %v", err),
			}
		}
	}

	// fetch content type
	contentType := strings.TrimSpace(h.Get("Content-Type"))
	if contentType == "" {
		contentType = types.ContentDefault
	}

	return types.ObjectStat{
		ObjectName:  objectName,
		ContentType: contentType,
		Size:        size,
	}, nil
}

// HeadObject query the objectInfo on chain to check th object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObject(ctx context.Context, bucketName, objectName string) (*storageTypes.ObjectInfo, error) {
	queryHeadObjectRequest := storageTypes.QueryHeadObjectRequest{
		BucketName: bucketName,
		ObjectName: objectName,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObject(ctx, &queryHeadObjectRequest)
	if err != nil {
		return nil, err
	}

	return queryHeadObjectResponse.ObjectInfo, nil
}

// HeadObjectByID query the objectInfo on chain by object id, return the object info if exists
// return err info if object not exist
func (c *client) HeadObjectByID(ctx context.Context, objID string) (*storageTypes.ObjectInfo, error) {
	headObjectRequest := storageTypes.QueryHeadObjectByIdRequest{
		ObjectId: objID,
	}
	queryHeadObjectResponse, err := c.chainClient.HeadObjectById(ctx, &headObjectRequest)
	if err != nil {
		return nil, err
	}

	return queryHeadObjectResponse.ObjectInfo, nil
}

// PutObjectPolicy apply object policy to the principal, return the txn hash
func (c *client) PutObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal,
	statements []*permTypes.Statement, opt types.PutPolicyOption,
) (string, error) {
	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)

	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	putPolicyMsg := storageTypes.NewMsgPutPolicy(c.MustGetDefaultAccount().GetAddress(), resource.String(),
		principal, statements, opt.PolicyExpireTime)

	return c.sendPutPolicyTxn(ctx, putPolicyMsg, opt.TxOpts)
}

// DeleteObjectPolicy delete the object policy of the principal
func (c *client) DeleteObjectPolicy(ctx context.Context, bucketName, objectName string, principalStr types.Principal, opt types.DeletePolicyOption) (string, error) {
	principal := &permTypes.Principal{}
	if err := principal.Unmarshal([]byte(principalStr)); err != nil {
		return "", err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	return c.sendDelPolicyTxn(ctx, c.MustGetDefaultAccount().GetAddress(), resource.String(), principal, opt.TxOpts)
}

// IsObjectPermissionAllowed check if the permission of the object is allowed to the user
func (c *client) IsObjectPermissionAllowed(ctx context.Context, userAddr string,
	bucketName, objectName string, action permTypes.ActionType,
) (permTypes.Effect, error) {
	_, err := sdk.AccAddressFromHexUnsafe(userAddr)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}
	verifyReq := storageTypes.QueryVerifyPermissionRequest{
		Operator:   userAddr,
		BucketName: bucketName,
		ObjectName: objectName,
		ActionType: action,
	}

	verifyResp, err := c.chainClient.VerifyPermission(ctx, &verifyReq)
	if err != nil {
		return permTypes.EFFECT_DENY, err
	}

	return verifyResp.Effect, nil
}

// GetObjectPolicy get the object policy info of the user specified by principalAddr
func (c *client) GetObjectPolicy(ctx context.Context, bucketName, objectName string, principalAddr string) (*permTypes.Policy, error) {
	_, err := sdk.AccAddressFromHexUnsafe(principalAddr)
	if err != nil {
		return nil, err
	}

	resource := gnfdTypes.NewObjectGRN(bucketName, objectName)
	queryPolicy := storageTypes.QueryPolicyForAccountRequest{
		Resource:         resource.String(),
		PrincipalAddress: principalAddr,
	}

	queryPolicyResp, err := c.chainClient.QueryPolicyForAccount(ctx, &queryPolicy)
	if err != nil {
		return nil, err
	}

	return queryPolicyResp.Policy, nil
}

// ListObjects return object list of the specific bucket
func (c *client) ListObjects(ctx context.Context, bucketName string, opts types.ListObjectsOptions) (types.ListObjectsResult, error) {
	if err := s3util.CheckValidBucketName(bucketName); err != nil {
		return types.ListObjectsResult{}, err
	}

	const listObjectsDefaultMaxKeys = 50
	if opts.MaxKeys == 0 {
		opts.MaxKeys = listObjectsDefaultMaxKeys
	}

	if opts.StartAfter != "" {
		if err := s3util.CheckValidObjectName(opts.StartAfter); err != nil {
			return types.ListObjectsResult{}, err
		}
	}

	if opts.ContinuationToken != "" {
		decodedContinuationToken, err := base64.StdEncoding.DecodeString(opts.ContinuationToken)
		if err != nil {
			return types.ListObjectsResult{}, err
		}
		opts.ContinuationToken = string(decodedContinuationToken)

		if err = s3util.CheckValidObjectName(opts.ContinuationToken); err != nil {
			return types.ListObjectsResult{}, err
		}

		if !strings.HasPrefix(opts.ContinuationToken, opts.Prefix) {
			return types.ListObjectsResult{}, fmt.Errorf("continuation-token does not match the input prefix")
		}
	}

	if ok := utils.IsValidObjectPrefix(opts.Prefix); !ok {
		return types.ListObjectsResult{}, fmt.Errorf("invalid object prefix")
	}

	params := url.Values{}
	params.Set("max-keys", strconv.FormatUint(opts.MaxKeys, 10))
	params.Set("start-after", opts.StartAfter)
	params.Set("continuation-token", opts.ContinuationToken)
	params.Set("delimiter", opts.Delimiter)
	params.Set("prefix", opts.Prefix)
	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return types.ListObjectsResult{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.ListObjectsResult{}, err
	}
	defer utils.CloseResponse(resp)

	// unmarshal the json content from response body
	buf := new(strings.Builder)
	_, err = io.Copy(buf, resp.Body)
	if err != nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	listObjectsResult := types.ListObjectsResult{}
	bufStr := buf.String()
	err = json.Unmarshal([]byte(bufStr), &listObjectsResult)
	// TODO(annie) remove tolerance for unmarshal err after structs got stabilized
	if err != nil && listObjectsResult.Objects == nil {
		log.Error().Msg("the list of objects in user's bucket:" + bucketName + " failed: " + err.Error())
		return types.ListObjectsResult{}, err
	}

	if opts.ShowRemovedObject {
		return listObjectsResult, nil
	}

	// default only return the object that has not been removed
	objectMetaList := make([]*types.ObjectMeta, 0)
	for _, objectInfo := range listObjectsResult.Objects {
		if objectInfo.Removed {
			continue
		}

		objectMetaList = append(objectMetaList, objectInfo)
	}

	listObjectsResult.Objects = objectMetaList
	listObjectsResult.KeyCount = strconv.Itoa(len(objectMetaList))
	return listObjectsResult, nil
}

// GetCreateObjectApproval returns the signature info for the approval of preCreating resources
func (c *client) GetCreateObjectApproval(ctx context.Context, createObjectMsg *storageTypes.MsgCreateObject) (*storageTypes.MsgCreateObject, error) {
	unsignedBytes := createObjectMsg.GetSignBytes()

	// set the action type
	urlValues := url.Values{
		"action": {types.CreateObjectAction},
	}

	reqMeta := requestMeta{
		urlValues:     urlValues,
		urlRelPath:    "get-approval",
		contentSHA256: types.EmptyStringSHA256,
		txnMsg:        hex.EncodeToString(unsignedBytes),
	}

	sendOpt := sendOptions{
		method:     http.MethodGet,
		isAdminApi: true,
	}

	bucketName := createObjectMsg.BucketName
	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		log.Error().Msg(fmt.Sprintf("route endpoint by bucket: %s failed, err: %s", bucketName, err.Error()))
		return nil, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return nil, err
	}

	// fetch primary signed msg from sp response
	signedRawMsg := resp.Header.Get(types.HTTPHeaderSignedMsg)
	if signedRawMsg == "" {
		return nil, errors.New("fail to fetch pre createObject signature")
	}

	signedMsgBytes, err := hex.DecodeString(signedRawMsg)
	if err != nil {
		return nil, err
	}

	var signedMsg storageTypes.MsgCreateObject
	storageTypes.ModuleCdc.MustUnmarshalJSON(signedMsgBytes, &signedMsg)

	return &signedMsg, nil
}

// CreateFolder send create empty object txn to greenfield chain
func (c *client) CreateFolder(ctx context.Context, bucketName, objectName string, opts types.CreateObjectOptions) (string, error) {
	if !strings.HasSuffix(objectName, "/") {
		return "", errors.New("failed to create folder. Folder names must end with a forward slash (/) character")
	}

	reader := bytes.NewReader([]byte(``))
	txHash, err := c.CreateObject(ctx, bucketName, objectName, reader, opts)
	return txHash, err
}

// GetObjectUploadProgress return the status of object including the uploading progress
func (c *client) GetObjectUploadProgress(ctx context.Context, bucketName, objectName string) (string, error) {
	status, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", err
	}

	// get object status from sp
	if status.ObjectStatus == storageTypes.OBJECT_STATUS_CREATED &&
		status.ObjectStatus != storageTypes.OBJECT_STATUS_SEALED {
		uploadProgressInfo, err := c.getObjectStatusFromSP(ctx, bucketName, objectName)
		if err != nil {
			return "", errors.New("fail to fetch object uploading progress from sp" + err.Error())
		}
		return uploadProgressInfo.ProgressDescription, nil
	}

	return status.ObjectStatus.String(), nil
}

func (c *client) getObjectStatusFromSP(ctx context.Context, bucketName, objectName string) (types.UploadProgress, error) {
	params := url.Values{}
	params.Set("upload-progress", "")

	reqMeta := requestMeta{
		urlValues:     params,
		bucketName:    bucketName,
		objectName:    objectName,
		contentSHA256: types.EmptyStringSHA256,
	}

	sendOpt := sendOptions{
		method:           http.MethodGet,
		disableCloseBody: true,
	}

	endpoint, err := c.getSPUrlByBucket(bucketName)
	if err != nil {
		return types.UploadProgress{}, err
	}

	resp, err := c.sendReq(ctx, reqMeta, &sendOpt, endpoint)
	if err != nil {
		return types.UploadProgress{}, err
	}

	defer utils.CloseResponse(resp)

	objectStatus := types.UploadProgress{}
	// decode the xml content from response body
	err = xml.NewDecoder(resp.Body).Decode(&objectStatus)
	if err != nil {
		return types.UploadProgress{}, err
	}

	return objectStatus, nil
}

func (c *client) UpdateObjectVisibility(ctx context.Context, bucketName, objectName string,
	visibility storageTypes.VisibilityType, opt types.UpdateObjectOption) (string, error) {
	objectInfo, err := c.HeadObject(ctx, bucketName, objectName)
	if err != nil {
		return "", fmt.Errorf("object:%s not exists: %s\n", objectName, err.Error())
	}

	if objectInfo.GetVisibility() == visibility {
		return "", fmt.Errorf("the visibility of object:%s is already %s \n", objectName, visibility.String())
	}

	updateObjectMsg := storageTypes.NewMsgUpdateObjectInfo(c.MustGetDefaultAccount().GetAddress(), bucketName, objectName, visibility)

	// set the default txn broadcast mode as sync mode
	if opt.TxOpts == nil {
		broadcastMode := tx.BroadcastMode_BROADCAST_MODE_SYNC
		opt.TxOpts = &gnfdsdk.TxOption{Mode: &broadcastMode}
	}

	return c.sendTxn(ctx, updateObjectMsg, opt.TxOpts)
}

// DownloadWorker download segment from sp
func DownloadWorker(id int, arg DownloadWorkerArg, jobs <-chan types.SegmentPiece, results chan<- types.SegmentPiece, failed chan<- error, die <-chan bool) {
	for seg := range jobs {
		if err := arg.hook(seg); err != nil {
			failed <- err
			break
		}

		// Resolve options
		s := fmt.Sprintf("bytes=%d-%d", seg.Start, seg.End)
		opts := arg.Options.GetObjectOption
		opts.Range = s

		rd, _, err := arg.Client.GetObject(arg.Ctx, arg.BucketName, arg.ObjectName, opts)
		if err != nil {
			failed <- err
			break
		}
		// TODO(chris)
		func() {
			defer rd.Close()
		}()

		select {
		case <-die:
			return
		default:
		}

		fd, err := os.OpenFile(arg.FilePath, os.O_CREATE|os.O_APPEND|os.O_WRONLY, types.FilePermMode)
		if err != nil {
			failed <- err
			break
		}

		newPosition, err := fd.Seek(seg.Start-seg.Offset, io.SeekStart)
		if err != nil {
			fd.Close()
			failed <- err
			break
		}

		startT := time.Now().UnixNano() / 1000 / 1000 / 1000

		_, err = io.Copy(fd, rd)
		log.Error().Msg(fmt.Sprintf("Range: %s, Seek, %dï¼Œ new oofset %d", s, seg.Start-seg.Offset, newPosition))

		endT := time.Now().UnixNano() / 1000 / 1000 / 1000
		if err != nil {
			// TODO(chris): request id
			log.Error().Msg(fmt.Sprintf("download seg error,cost:%d second,seg number:%d,request id:%s,error:%s.\n", endT-startT, seg.Index, "0", err.Error()))
			fd.Close()
			failed <- err
			break
		}

		// TODO(chris): crc
		//if arg.EnableCRC {
		//	//seg.CRC64 = crcCalc.Sum64()
		//}

		fd.Close()
		results <- seg
	}
}

// DownloadScheduler
func DownloadScheduler(jobs chan types.SegmentPiece, segs []types.SegmentPiece) {
	for _, seg := range segs {
		jobs <- seg
	}
	close(jobs)
}
